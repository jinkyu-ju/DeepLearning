{"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mc67GZ10vhXe","executionInfo":{"status":"ok","timestamp":1643944820136,"user_tz":-540,"elapsed":320,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"75865456-e05d-45bc-8706-1da5ed104001"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fri Feb  4 03:20:19 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"MYugMo07Ny0n"},"source":["# 2.2 Data representations for neural networks"]},{"cell_type":"markdown","metadata":{"id":"-KVK11R6Ny0p"},"source":["- In the previous example, we started from data stored in **multidimensional Numpy arrays**, also called **tensors**. \n","- In general, all current machine-learning systems use **tensors as their basic data structure**. \n","- Tensors are fundamental to the field—so fundamental that Google’s TensorFlow was named after them. \n","\n","## So what’s a tensor?"]},{"cell_type":"markdown","metadata":{"id":"QHCmh21TNy0q"},"source":["- At its core, <font color=\"blue\">**a tensor is a container for data**</font>\n","— Almost always numerical data. So, it’s <font color=\"blue\">**a container for numbers**</font> \n","- You may be already familiar with **matrices**, which are **2D tensors** \n","- Tensors are a generalization of matrices to an arbitrary number of dimensions  \n","(**임의의 차원 개수를 가지는 행렬의 일바화된 모습임**)\n","- Note that in the context of tensors, **a dimension** is often called **an axis**"]},{"cell_type":"markdown","metadata":{"id":"lee7NOMZNy0q"},"source":["## 2.2.1. Scalars (0D tensors)\n","\n","- <font color=\"blue\">**tensor that contains only one number**</font> is called a **scalar (or scalar tensor, or 0-dimensional tensor, or 0D tensor)**. \n","- In Numpy, a float32 or float64 number is a scalar tensor (or scalar array). \n","- You can display <U>the number of axes of a Numpy tensor via <B>the <font color=\"blue\">ndim</font> attribute</B></U>; a scalar tensor has 0 axes (ndim == 0). \n","- **The number of axes of a tensor** is also called its **rank**.\n","\n","### Here’s a Numpy scalar:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Aujf65PNy0r","executionInfo":{"status":"ok","timestamp":1643944991902,"user_tz":-540,"elapsed":235,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"dab5f3cb-c7ea-497e-afc8-a5962b133d1d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(12)"]},"metadata":{},"execution_count":4}],"source":["import numpy as np\n","x = np.array(12)\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rF5_nASwNy0s","executionInfo":{"status":"ok","timestamp":1643944993657,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"1d36b6de-13c9-44ef-bb8e-d0b6dd44762d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":5}],"source":["x.ndim"]},{"cell_type":"markdown","metadata":{"id":"M2h0_x0hNy0s"},"source":["## Vectors (1D tensors)\n","\n","- **An array of numbers** is called **a vector**, or **1D tensor**. \n","- A 1D tensor is said to have exactly one axis. \n","\n","### Following is a Numpy vector:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLWZ2d5CNy0s","executionInfo":{"status":"ok","timestamp":1643944996916,"user_tz":-540,"elapsed":233,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"53935cde-57dc-437e-e9d5-be34cfa1e3f0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([12,  3,  6, 14,  7])"]},"metadata":{},"execution_count":6}],"source":["x = np.array([12, 3, 6, 14, 7])\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxD5dWbbNy0t","executionInfo":{"status":"ok","timestamp":1643945000888,"user_tz":-540,"elapsed":235,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"214722b6-31d9-4ffe-f55f-6fe3af1219af"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":7}],"source":["x.ndim"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Bgif22LNy0t","executionInfo":{"status":"ok","timestamp":1643945002303,"user_tz":-540,"elapsed":244,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"f5e1269e-7e5b-41e8-9743-0d85a2631430"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(5,)"]},"metadata":{},"execution_count":8}],"source":["x.shape"]},{"cell_type":"markdown","metadata":{"id":"CRO89IK4Ny0u"},"source":["- This vector has five entries and so is called a **5-dimensional vector**. \n","- <font color='blue'>**Don’t confuse a 5D vector with a 5D tensor!**</font> \n","- A 5D vector has only one axis and has five dimensions along its axis, whereas a **5D tensor has five axes** (and may have any number of dimensions along each axis).  \n","(5D 벡터는 하나의 축을 따라 5개의 차원을 가진 것이고 5D 텐서는 5개의 축을 가진 것 (텐서의 각 축을 따라 여러 개의 차원을 가진 벡터가 놓일 수 있음))  \n","5D tensor는 4D tensor의 배열임.\n","- Dimensionality can denote either the number of entries along a specific axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a 5D tensor), which can be confusing at times. \n","- In the latter case, it’s technically more correct to talk about **a tensor of rank 5** (the rank of a tensor being the number of axes), but the ambiguous notation 5D tensor is common regardless."]},{"cell_type":"markdown","metadata":{"id":"XU9Pk1s7Ny0u"},"source":["## Matrices (2D tensors)\n","\n","- An array of vectors is **a matrix, or 2D tensor**. \n","- A matrix has two axes (often referred to rows and columns). \n","- You can visually interpret a matrix as a rectangular grid of numbers. \n","\n","### This is a Numpy matrix:"]},{"cell_type":"markdown","metadata":{"id":"9bBW0wX8Ny0v"},"source":["- The entries from **the first axis** are called the **rows**, and the entries from **the second axis** are called the **columns**. \n","- In the previous example, [5, 78, 2, 34, 0] is the first row of x, and [5, 6, 7] is the first column."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QD1GoeufNy0u","executionInfo":{"status":"ok","timestamp":1643945006430,"user_tz":-540,"elapsed":234,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"22d719f7-2122-4a87-e629-7e0672983082"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{},"execution_count":9}],"source":["x = np.array([[5, 78, 2, 34, 0],\n","              [6, 79, 3, 35, 1],\n","              [7, 80, 4, 36, 2]])\n","x.ndim"]},{"cell_type":"code","source":["x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zoyeHrjDwiaU","executionInfo":{"status":"ok","timestamp":1643945024303,"user_tz":-540,"elapsed":231,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"75e81f7f-4109-41ac-ee93-fc82bdf703d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 5)"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"jWeGUkOkNy0v"},"source":["## 2.2.4. 3D tensors and higher-dimensional tensors\n","\n","If you pack such matrices in a new array, you obtain a 3D tensor, which you can visually interpret as a cube of numbers.  \n","\n","### Following is a Numpy 3D tensor:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7abd32FzNy0v","executionInfo":{"status":"ok","timestamp":1643945070525,"user_tz":-540,"elapsed":236,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"6eac25eb-b055-4f80-b07e-97edf0f40600"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":12}],"source":["x = np.array([[[5, 78, 2, 34, 0],\n","               [6, 79, 3, 35, 1],\n","               [7, 80, 4, 36, 2]],\n","              [[5, 78, 2, 34, 0],\n","               [6, 79, 3, 35, 1],\n","               [7, 80, 4, 36, 2]],\n","              [[5, 78, 2, 34, 0],\n","               [6, 79, 3, 35, 1],\n","               [7, 80, 4, 36, 2]]])\n","x.ndim\n"]},{"cell_type":"code","source":["x.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3j3kqlLKwvdN","executionInfo":{"status":"ok","timestamp":1643945075945,"user_tz":-540,"elapsed":3,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"1c9cd9c7-5d88-4daa-8eda-6947eed05753"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3, 3, 5)"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"iSpD61qfNy0v"},"source":["- By packing 3D tensors in an array, you can create a 4D tensor, and so on. \n","- In deep learning, you’ll generally manipulate tensors that are 0D to 4D, although you may go up to 5D if you process video data."]},{"cell_type":"markdown","metadata":{"id":"BMhX7ctBNy0w"},"source":["## 2.2.5. Key attributes\n","\n","A tensor is defined by three key attributes:\n","\n","- **Number of axes (rank)**— For instance, a 3D tensor has three axes, and a matrix has two axes. This is also called the tensor’s ndim in Python libraries such as Numpy.\n","- **Shape**— This is a tuple of integers that describes how many dimensions the tensor has along each axis. For instance, the previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5). A vector has a shape with a single element, such as (5,), whereas a scalar has an empty shape, ().\n","- **Data type** (usually called dtype in Python libraries)—This is the type of the data contained in the tensor; for instance, a tensor’s type could be float32, uint8, float64, and so on. On rare occasions, you may see a char tensor. Note that string tensors don’t exist in Numpy (or in most other libraries), because tensors live in preallocated, contiguous memory segments: and strings, being variable length, would preclude the use of this implementation."]},{"cell_type":"markdown","metadata":{"id":"jg2ZjlOSNy0w"},"source":["To make this more concrete, let’s look back at the data we processed in the MNIST example. \n","\n","### First, we load the MNIST dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l6iaCwSWNy0w","executionInfo":{"status":"ok","timestamp":1670562743096,"user_tz":-540,"elapsed":5632,"user":{"displayName":"김하영","userId":"15226092368577588417"}},"outputId":"7e717f50-761d-4d42-b393-0b56642602d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11490434/11490434 [==============================] - 0s 0us/step\n"]}],"source":["%matplotlib inline\n","from keras.datasets import mnist\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"]},{"cell_type":"markdown","metadata":{"id":"vYMqs_hFNy0w"},"source":["Next, we display the number of axes of the tensor train_images, the ndim attribute:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFMq726HNy0x","executionInfo":{"status":"ok","timestamp":1670562743097,"user_tz":-540,"elapsed":13,"user":{"displayName":"김하영","userId":"15226092368577588417"}},"outputId":"4a556853-5535-428a-efc0-b19b4e318029"},"outputs":[{"output_type":"stream","name":"stdout","text":["3\n"]}],"source":["print(train_images.ndim)"]},{"cell_type":"markdown","metadata":{"id":"Sv7W40ohNy0x"},"source":["Here’s its shape:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zNi7BJJxNy0x","executionInfo":{"status":"ok","timestamp":1670562743097,"user_tz":-540,"elapsed":6,"user":{"displayName":"김하영","userId":"15226092368577588417"}},"outputId":"d70c1fc2-d36e-4b25-f340-bd890351acde"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n"]}],"source":["print(train_images.shape)"]},{"cell_type":"markdown","metadata":{"id":"Gu-RfGjTNy0x"},"source":["And this is its data type, the dtype attribute:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1x6yVll-Ny0x","executionInfo":{"status":"ok","timestamp":1670562743097,"user_tz":-540,"elapsed":5,"user":{"displayName":"김하영","userId":"15226092368577588417"}},"outputId":"63f71ee2-a655-422d-d7fa-b100da39b5ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["uint8\n"]}],"source":["print(train_images.dtype)"]},{"cell_type":"markdown","metadata":{"id":"Wrep-4sENy0x"},"source":["- So what we have here is a 3D tensor of 8-bit integers. \n","- More precisely, it’s an array of 60,000 matrices of 28 × 28 integers. \n","- Each such matrix is a grayscale image, with coefficients between 0 and 255."]},{"cell_type":"markdown","metadata":{"id":"piJqZ80QNy0x"},"source":["Let’s display the fourth digit in this 3D tensor, using the library Matplotlib (part of the standard scientific Python suite); see figure 2.2."]},{"cell_type":"markdown","metadata":{"id":"fNliiqeRNy0y"},"source":["Figure 2.2. The fourth sample in our dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"zZSo6JLLNy0y","executionInfo":{"status":"ok","timestamp":1670562743584,"user_tz":-540,"elapsed":490,"user":{"displayName":"김하영","userId":"15226092368577588417"}},"outputId":"0eefe72d-7761-47e4-e22a-a7cee8dce891"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["digit = train_images[4]\n","import matplotlib.pyplot as plt\n","plt.imshow(digit, cmap=plt.cm.binary)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"sDo0vFYNNy0y"},"source":["## 2.2.6. Manipulating tensors in Numpy\n","\n","- In the previous example, we selected a specific digit alongside the first axis using the syntax train_images[i]. \n","- **Selecting specific elements in a tensor** is called **tensor slicing**. \n","\n","### Let’s look at the tensor-slicing operations you can do on Numpy arrays."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VSAV73snNy0y","executionInfo":{"status":"ok","timestamp":1642747472182,"user_tz":-540,"elapsed":12,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"20a98d39-00e0-4f71-b9c8-4a1f183bcb3c"},"outputs":[{"output_type":"stream","name":"stdout","text":["(90, 28, 28)\n"]}],"source":["my_slice = train_images[10:100]\n","print(my_slice.shape)"]},{"cell_type":"markdown","metadata":{"id":"g7NsTSjtNy0y"},"source":["It’s equivalent to this more detailed notation, which specifies a start index and stop index for the slice along each tensor axis. Note that : is equivalent to selecting the entire axis:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_VuhvC5Ny0y","executionInfo":{"status":"ok","timestamp":1642747472183,"user_tz":-540,"elapsed":11,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"2da0853e-3040-4d8a-e2b5-9fa462bc8483"},"outputs":[{"output_type":"stream","name":"stdout","text":["(90, 28, 28)\n"]}],"source":["my_slice = train_images[10:100, :, :]\n","print(my_slice.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nt27CCxvNy0y","executionInfo":{"status":"ok","timestamp":1642747472183,"user_tz":-540,"elapsed":10,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"379c75b0-baa6-4a6d-9efd-86752f90cb28"},"outputs":[{"output_type":"stream","name":"stdout","text":["(90, 28, 28)\n"]}],"source":["my_slice = train_images[10:100, 0:28, 0:28]\n","print(my_slice.shape)"]},{"cell_type":"markdown","metadata":{"id":"bealOf81Ny0y"},"source":["- In general, you may select between any two indices along each tensor axis. \n","- For instance, in order to select 14 × 14 pixels in the bottom-right corner of all images, you do this:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mbtgHmR7Ny0z","executionInfo":{"status":"ok","timestamp":1642747472184,"user_tz":-540,"elapsed":10,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"f1df6168-352b-4cbe-c746-e1ae52b804ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 14, 14)\n"]}],"source":["my_slice2 = train_images[:, 14:, 14:]\n","print(my_slice2.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"1W3GHnj9Ny0z","executionInfo":{"status":"ok","timestamp":1642747472185,"user_tz":-540,"elapsed":10,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"7cba0838-44bd-45fd-907a-b74d16de7ddc"},"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAMFUlEQVR4nO3dXaxldXnH8e+PmWGUERgok8k4A+U1NBNiizkxqI1thCYjAmNIL4ZIA5WEm7aiMTHAXEjvmmiIJoiGIEgqYUIQKyFqmaJimlTi4SUUGJQXKQwMzhBTBb2AiU8v9sYcTmaArrX2Ogf+309ycvZae//P85yd+c162Wudf6oKSe98hyx1A5LGYdilRhh2qRGGXWqEYZcasXLMYknetqf+Tz755M5jjzzyyAE7kQ7u6aef5sUXX8yBnhs17ADJAfuY+dhDDum3E3P11Vd3Hnvuuef2qi29VXNzcwd9zt14qRGGXWqEYZca0SvsSbYk+XmSJ5JcPlRTkobXOexJVgBfBT4GbAYuSLJ5qMYkDavPlv0DwBNV9VRVvQLsALYO05akofUJ+0bg2QXLu6frXifJpUnmk8z3qCWpp5l/zl5V1wHXwdv7ohrp7a7Plv054NgFy5um6yQtQ33C/jPglCQnJDkU2AbcMUxbkobWeTe+qvYn+Ufg34EVwA1V9chgnUkaVK9j9qr6HvC9gXqRNENeQSc1wrBLjRj1FtdVq1axfv36zuOff/75zmOPOeaYzmPB21T19ueWXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaMeotrqtXr+akk07qPL7PLa7btm3rPFZ6J3DLLjXCsEuNMOxSIwy71Ig+s7gem+RHSR5N8kiSy4ZsTNKw+pyN3w98rqruT3I4cF+SnVX16EC9SRpQ5y17Ve2pqvunj18CdnGAWVwlLQ+DfM6e5HjgdODeAzx3KXApTD5nl7Q0ep+gS/Ie4NvAZ6rqt4ufr6rrqmququZWrVrVt5ykjnqFPckqJkG/uapuH6YlSbPQ52x8gG8Au6rq6uFakjQLfbbsHwb+DvhokgenX2cP1JekgfWZn/0/gQzYi6QZ8go6qRGGXWrEqPezv/zyy9xzzz2dx0/OCXZz4okndh4rvRO4ZZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRhh2qRGGXWqEYZcaYdilRox6iyv0u021z1inbFbr3LJLjTDsUiMMu9QIwy41Yojpn1YkeSDJnUM0JGk2htiyX8ZkBldJy1jfud42AR8Hrh+mHUmz0nfL/mXg88AfDvaCJJcmmU8y37OWpB76TOx4DrC3qu57o9ctnLK5ay1J/fWd2PG8JE8DO5hM8PitQbqSNLjOYa+qK6pqU1UdD2wDflhVFw7WmaRB+Tm71IhBboSpqh8DPx7iZ0maDbfsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9SIvhM7rk1yW5LHkuxK8sGhGpM0rL5/N/4rwA+q6m+THAocNkBPkmagc9iTHAl8BLgYoKpeAV4Zpi1JQ+uzG38CsA+4MckDSa5Psmbxi5yyWVoe+oR9JfB+4GtVdTrwO+DyxS9yymZpeegT9t3A7qq6d7p8G5PwS1qG+kzZ/ALwbJJTp6vOBB4dpCtJg+t7Nv6fgJunZ+KfAv6+f0uSZqFX2KvqQcBjceltwCvopEYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRff8G3f9bVY1dUhJu2aVmGHapEYZdakTfKZs/m+SRJA8nuSXJu4ZqTNKwOoc9yUbg08BcVZ0GrAC2DdWYpGH13Y1fCbw7yUomc7M/378lSbPQZ66354AvAc8Ae4DfVNVdi1/nlM3S8tBnN/4oYCuTedrfC6xJcuHi1zlls7Q89NmNPwv4ZVXtq6pXgduBDw3TlqSh9Qn7M8AZSQ5LEiZTNu8api1JQ+tzzH4vcBtwP/Df05913UB9SRpY3ymbvwB8YaBeJM2QV9BJjTDsUiNGvcV19erVHHfccZ3HP/nkk0syFmDdunW9xktLzS271AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNMOxSIwy71AjDLjXCsEuNGPV+9g0bNnDllVd2Hn/JJZd0HtunLsA111zTeezmzZt71ZaG4JZdaoRhlxph2KVGvGnYk9yQZG+ShxesOzrJziSPT78fNds2JfX1Vrbs3wS2LFp3OXB3VZ0C3D1dlrSMvWnYq+onwK8Xrd4K3DR9fBPwiYH7kjSwrsfs66tqz/TxC8D6g71w4ZTNL730UsdykvrqfYKuqgqoN3j+j1M2H3744X3LSeqoa9h/lWQDwPT73uFakjQLXcN+B3DR9PFFwHeHaUfSrLyVj95uAf4LODXJ7iSXAP8C/E2Sx4GzpsuSlrE3vTa+qi44yFNnDtyLpBnyCjqpEYZdasSot7iuXbuW888/v/P4HTt2dB67c+fOzmMBrrrqqs5jb7zxxl6116xZ02u8BG7ZpWYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qhGGXGmHYpUYYdqkRhl1qxKj3s69YsYIjjjii8/hbb72189jt27d3Hgtw7bXXdh7b5154cMpnDcMtu9QIwy41wrBLjeg6ZfMXkzyW5KEk30mydrZtSuqr65TNO4HTqup9wC+AKwbuS9LAOk3ZXFV3VdX+6eJPgU0z6E3SgIY4Zv8U8P0Bfo6kGeoV9iTbgf3AzW/wmj/Oz75v374+5ST10DnsSS4GzgE+OZ2j/YAWzs++bt26ruUk9dTpCrokW4DPA39VVb8ftiVJs9B1yuZrgMOBnUkeTPL1GfcpqaeuUzZ/Ywa9SJohr6CTGmHYpUbkDU6kD25ubq7m5+dHqye1Zm5ujvn5+RzoObfsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41wrBLjTDsUiMMu9QIwy41YtT72ZPsA/7nDV5yDPDiSO1Y29rvxNp/WlUH/DPOo4b9zSSZr6o5a1vb2sNzN15qhGGXGrHcwn6dta1t7dlYVsfskmZnuW3ZJc2IYZcasSzCnmRLkp8neSLJ5SPWPTbJj5I8muSRJJeNVXtBDyuSPJDkzpHrrk1yW5LHkuxK8sERa392+n4/nOSWJO+acb0bkuxN8vCCdUcn2Znk8en3o0as/cXp+/5Qku8kWTuL2ostediTrAC+CnwM2AxckGTzSOX3A5+rqs3AGcA/jFj7NZcBu0auCfAV4AdV9WfAn4/VQ5KNwKeBuao6DVgBbJtx2W8CWxatuxy4u6pOAe6eLo9VeydwWlW9D/gFcMWMar/Okocd+ADwRFU9VVWvADuArWMUrqo9VXX/9PFLTP7BbxyjNkCSTcDHgevHqjmteyTwEaYTdFbVK1X1vyO2sBJ4d5KVwGHA87MsVlU/AX69aPVW4Kbp45uAT4xVu6ruqqr908WfAptmUXux5RD2jcCzC5Z3M2LgXpPkeOB04N4Ry36ZyTz3fxixJsAJwD7gxukhxPVJ1oxRuKqeA74EPAPsAX5TVXeNUXuR9VW1Z/r4BWD9EvQA8Cng+2MUWg5hX3JJ3gN8G/hMVf12pJrnAHur6r4x6i2yEng/8LWqOh34HbPbjX2d6bHxVib/4bwXWJPkwjFqH0xNPn8e/TPoJNuZHErePEa95RD254BjFyxvmq4bRZJVTIJ+c1XdPlZd4MPAeUmeZnLo8tEk3xqp9m5gd1W9thdzG5Pwj+Es4JdVta+qXgVuBz40Uu2FfpVkA8D0+94xiye5GDgH+GSNdLHLcgj7z4BTkpyQ5FAmJ2vuGKNwkjA5bt1VVVePUfM1VXVFVW2qquOZ/M4/rKpRtnBV9QLwbJJTp6vOBB4dozaT3fczkhw2ff/PZGlOUN4BXDR9fBHw3bEKJ9nC5PDtvKr6/Vh1qaol/wLOZnJW8klg+4h1/5LJ7ttDwIPTr7OX4Pf/a+DOkWv+BTA//d3/DThqxNr/DDwGPAz8K7B6xvVuYXJ+4FUmezWXAH/C5Cz848B/AEePWPsJJuepXvs39/Ux3ncvl5UasRx24yWNwLBLjTDsUiMMu9QIwy41wrBLjTDsUiP+D6TvYUtPAqI4AAAAAElFTkSuQmCC\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["digit2 = my_slice2[4]\n","#import matplotlib.pyplot as plt\n","plt.imshow(digit2, cmap=plt.cm.binary)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"z03yfmBwNy0z"},"source":["- It’s also possible to use negative indices. \n","- Much like negative indices in Python lists, they indicate a position relative to the end of the current axis. \n","- In order to crop the images to patches of 14 × 14 pixels centered in the middle, you do this:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IeIvQTM5Ny0z","executionInfo":{"status":"ok","timestamp":1642747472186,"user_tz":-540,"elapsed":10,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"719fd48e-3188-49e4-e6d2-808958cb4d63"},"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 14, 14)\n"]}],"source":["my_slice3 = train_images[:, 7:-7, 7:-7]\n","print(my_slice3.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"mXOXe7iTNy0z","executionInfo":{"status":"ok","timestamp":1642747472699,"user_tz":-540,"elapsed":4,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"b0916e8f-18d8-4424-8435-8a45147c002c"},"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["digit3 = my_slice3[4]\n","plt.imshow(digit3, cmap=plt.cm.binary)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_UOM7_DGNy0z"},"source":["## 2.2.7. The notion of data batches\n","\n","- In general, <font color=\"blue\">**the first axis (axis 0, because indexing starts at 0)** in all data tensors you’ll come across in deep learning will be **the samples axis (sometimes called the samples dimension)**</font>. \n","- In the MNIST example, samples are images of digits.\n","\n","- In addition, deep-learning models don’t process an entire dataset at once; rather, they break the data into small batches. Concretely, here’s one batch of our MNIST digits, with batch size of 128:"]},{"cell_type":"markdown","metadata":{"id":"xwPTVYjfNy0z"},"source":["batch = train_images[:128]"]},{"cell_type":"markdown","metadata":{"id":"26ufU3f8Ny00"},"source":["And here’s the next batch:"]},{"cell_type":"markdown","metadata":{"id":"gYti6UtwNy00"},"source":["batch = train_images[128:256]"]},{"cell_type":"markdown","metadata":{"id":"uvO7x69uNy00"},"source":["And the nth batch:"]},{"cell_type":"markdown","metadata":{"id":"05wMq4JLNy00"},"source":["batch = train_images[128 * n:128 * (n + 1)]"]},{"cell_type":"markdown","metadata":{"id":"TKAqFkn2Ny00"},"source":["- When considering such a batch tensor, the first axis (axis 0) is called the <font color=\"blue\">**batch axis** or **batch dimension**</font>. \n","- This is a term you’ll frequently encounter when using Keras and other deep-learning libraries."]},{"cell_type":"markdown","metadata":{"id":"m3NnEMwiNy00"},"source":["## 2.2.8. Real-world examples of data tensors\n","\n","Let’s make data tensors more concrete with a few examples similar to what you’ll encounter later. The data you’ll manipulate will almost always fall into one of the following categories:\n","\n","- <font color=\"blue\">**Vector data**</font>— 2D tensors of shape <font color=\"blue\">(samples, features)</font>\n","- <font color=\"blue\">**Timeseries data or sequence data**</font>— 3D tensors of shape <font color=\"blue\">(samples, timesteps, features)</font>\n","- <font color=\"blue\">**Images**</font>— 4D tensors of shape <font color=\"blue\">(samples, height, width, channels)</font> or <font color=\"blue\">(samples, channels, height, width)</font>\n","- <font color=\"blue\">**Video**</font>— 5D tensors of shape <font color=\"blue\">(samples, frames, height, width, channels)</font> or <font color=\"blue\">(samples, frames, channels, height, width)</font>"]},{"cell_type":"markdown","metadata":{"id":"-G3OkX19Ny00"},"source":["## 2.2.9. Vector data\n","\n","- This is the most common case. \n","- In such a dataset, each single data point can be encoded as a vector\n","- A batch of data will be encoded as a 2D tensor (that is, an array of vectors), \n","  where the **first axis** is the **samples axis** and the **second axis** is the **features axis**."]},{"cell_type":"markdown","metadata":{"id":"5GiavNjdNy00"},"source":["Let’s take a look at two examples:\n","\n","- An actuarial dataset of people, where we consider **each person’s age, ZIP code, and income**.  \n","Each person can be characterized as **a vector of 3 values**, and thus an entire dataset of **100,000 people** can be stored in a  \n"," **2D tensor of shape (100000, 3)**.\n","\n","- A dataset of text documents, where we represent each document by the counts of how many times each word appears in it (**out of a dictionary of 20,000 common words**).  \n","Each document can be encoded as a vector of 20,000 values (one count per word in the dictionary), and thus an entire dataset of **500 documents** can be stored in a tensor of shape  \n"," **(500, 20000)**.\n","    \n","    "]},{"cell_type":"markdown","metadata":{"id":"OJ1XrapNNy00"},"source":["## 2.2.10. Timeseries data or sequence data\n","\n","- Whenever time matters in your data (or the notion of sequence order), it makes sense to store it in a 3D tensor with an explicit time axis. \n","- Each sample can be encoded as a sequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D tensor (see figure 2.3)."]},{"cell_type":"markdown","source":["<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=1i9jixQB3aYLhaU5Cvh_sOkV0l5Au3MXY\" width=\"800\"/>\n","</div>"],"metadata":{"id":"Eilk8U1VsBMD"}},{"cell_type":"markdown","metadata":{"id":"sbx_kKaXNy02"},"source":["The time axis is always the second axis (axis of index 1), by convention:  \n","**<font color=\"blue\">(samples, timesteps, features)</font>** \n","\n","Let’s look at a few examples:\n","- A dataset of stock prices.  \n"," **Every minute**, we store **the current price** of the stock, **the highest price** in the past minute, and **the lowest price** in the past minute.  \n"," Thus <U>every minute is encoded as a 3D vector</U>, **an entire day of trading** is encoded as a 2D tensor of shape **(390, 3)** (there are 390 minutes in a trading day), and **250 days’ worth of data** can be stored in a 3D tensor of shape **(250, 390, 3)**.  \n"," Here, each sample would be one day’s worth of data.\n","\n","- A dataset of tweets, where we encode <U>each tweet as a sequence of 280 characters out of an alphabet of 128 unique characters</U>.  \n","In this setting, each character can be encoded as a binary vector of size 128 (an all-zeros vector except for a 1 entry at the index corresponding to the character).  \n","Then **each tweet** can be encoded as a 2D tensor of shape **(280, 128)**, and a dataset of **1 million tweets** can be stored in a tensor of shape **(1000000, 280, 128)**."]},{"cell_type":"markdown","metadata":{"id":"miSlUawaNy02"},"source":["## 2.2.11. Image data\n","\n","- Images typically have <U>three dimensions</U>: **height, width, and color depth**.  \n","- Although grayscale images (like our MNIST digits) have only a single color channel and could thus be stored in 2D tensors, by convention image tensors are always 3D, with a one-dimensional color channel for grayscale images.  \n","For example, (28, 28, **<font color=\"blue\">1</font>**)  \n","- **A batch of 128 grayscale images of size 256 × 256** could thus be stored in a tensor of shape **(128, 256, 256, 1)**,  \n","and **a batch of 128 color images** could be stored in a tensor of shape **(128, 256, 256, 3)** (see figure 2.4)."]},{"cell_type":"markdown","source":["<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=18zrZ9Bvr9nupwPthxC89saZd0PwvVgxt\" width=\"300\"/>\n","</div>"],"metadata":{"id":"kPQaOP1UsNOq"}},{"cell_type":"markdown","metadata":{"id":"DvNn1uU0Ny02"},"source":["There are two conventions for shapes of images tensors: \n","- the **channels-last convention** (used by TensorFlow) and the **channels-first convention** (used by Theano). \n","- The TensorFlow machine-learning framework, from Google, places the color-depth axis at the end: **(samples, height, width, color_depth)**. \n","- Meanwhile, Theano places the color depth axis right after the batch axis: (samples, color_depth, height, width). \n","- With the Theano convention, the previous examples would become (128, 1, 256, 256) and (128, 3, 256, 256). \n","- The Keras framework provides support for both formats."]},{"cell_type":"markdown","metadata":{"id":"p6n-PmrGNy03"},"source":["## 2.2.12. Video data\n","\n","- Video data is one of the few types of real-world data for which you’ll need 5D tensors.  \n","(비디오 데이터는 현실에서 5D 텐서가 필요한 몇 안되는 데이터 중 하나임)\n","- **A video** can be understood as **a sequence of frames**, **each frame** being **a color image**.  \n","(**하나의 비디오는 프레임의 연속이고 각 프레임은 하나의 컬러 이미지임**) \n","- Because each frame can be stored in a 3D tensor (height, width, color_depth), a sequence of frames can be stored in a 4D tensor (frames, height, width, color_depth), and thus a batch of different videos can be stored in a 5D tensor of shape (samples, frames, height, width, color_depth).  \n","(프레임이 (height, width, color_depth)의 3D 텐서로 저장될 수 있기 때문에  \n","프레임의 연속은 (frames, height, width, color_depth)의 4D 텐서로 저장됨  \n","여러 비디오의 배치는 (samples, frames, height, width, color_depth)의 5D 텐서로 저장됨.)"]},{"cell_type":"markdown","metadata":{"id":"chqcq9HONy03"},"source":["- For instance, a 60-second, 144 × 256 YouTube video clip sampled at 4 frames per second would have 240 frames. \n","- A batch of four such video clips would be stored in a tensor of shape (4, 240, 144, 256, 3). \n","- That’s a total of 106,168,320 values! \n","- If the dtype of the tensor was float32, then each value would be stored in 32 bits, so the tensor would represent 405 MB. Heavy! \n","- Videos you encounter in real life are much lighter, because they aren’t stored in float32, and they’re typically compressed by a large factor (such as in the MPEG format)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"colab":{"provenance":[],"collapsed_sections":["jg2ZjlOSNy0w"]}},"nbformat":4,"nbformat_minor":0}