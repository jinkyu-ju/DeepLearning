{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"YjBITPySOnkD","executionInfo":{"status":"ok","timestamp":1643945215111,"user_tz":-540,"elapsed":5624,"user":{"displayName":"‍김하영(전임교원/정보대학원)","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjZ-U08qs4TBUqGIgnkbm3YhProsH2j7QBI_Bem=s64","userId":"15226092368577588417"}},"outputId":"3cd31544-a167-4f36-ac43-cde710eca2a1"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.7.0'"]},"metadata":{},"execution_count":1}],"source":["import keras\n","keras.__version__"]},{"cell_type":"markdown","metadata":{"id":"G_zBxixoOnkG"},"source":["# 2.3. The gears of neural networks: tensor operations\n","\n","- All transformations learned by deep neural networks can be reduced to   \n","a handful of **tensor operations** applied to tensors of numeric data.  \n","(심층 신경망이 학습한 모든 변환을 **수치 데이터 텐서에 적용하는 몇 종류의 텐서 연산**으로 나타낼 수 있음)\n","- For instance, it’s possible to **add tensors, multiply tensors**, and so on.\n"]},{"cell_type":"markdown","metadata":{"id":"LXqEytbAOnkH"},"source":["In our initial example, we were building our network by **stacking Dense layers** on top of each other. \n","\n","A Keras layer instance looks like this:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7D5-Uv-POnkH"},"outputs":[],"source":["keras.layers.Dense(512, activation='relu')"]},{"cell_type":"markdown","metadata":{"id":"q2PJws93OnkI"},"source":["- **This layer** can be interpreted as a **function**,  \n","which takes as **input a 2D tensor** and **returns another 2D tensor**  \n"," —**<font color=\"blue\">a new representation for the input tensor**</font>.  \n","(이 층은 **2D 텐서를 입력**으로 받고  \n"," **입력 텐서의 새로운 표현**인 **또 다른 2D 텐서를 반환**하는 **함수**처럼 해석할 수 있음)  \n","\n","         y=f(x)  \n","         New representations = dense(input tensor)\n","\n"]},{"cell_type":"markdown","source":["- Specifically, the function is as follows (where W is a 2D tensor and b is a vector, both attributes (parameters) of the layer):  \n","(함수는 다음과 같음 (**W는 2D 텐서, b는 벡터, 둘 모두 layer(층)의 속성**임))\n","\n","**output = relu(dot(W, input) + b)**"],"metadata":{"id":"OU7Jq1ZpAqfz"}},{"cell_type":"markdown","metadata":{"id":"2Rtlntn-OnkI"},"source":["Let’s unpack this. \n","\n","**We have three tensor operations here:** \n","- a **dot product (dot)** between the input tensor and a tensor named W; \n","- an **addition (+)** between the resulting 2D tensor and a vector b; \n","- and, finally, **a relu operation**. relu(x) is max(x, 0)."]},{"cell_type":"markdown","metadata":{"id":"kup1MBAeOnkJ"},"source":["## 2.3.1. Element-wise operations\n","\n","- The **relu operation** and **addition** are **element-wise operations**: \n","- That is, operations that are applied independently to each entry in the tensors being considered.  \n","(이 연산은 텐서에 있는 **각 원소에 독립적으로 적용**됨)\n","- This means these operations are **highly amenable to massively parallel implementations**  \n","(vectorized implementations, a term that comes from the vector processor supercomputer architecture from the 1970–1990 period).  \n","(이 말은 **고도의 병렬 구현이 가능**한 연산이라는 의미임.) \n"]},{"cell_type":"markdown","source":["<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=1kYZcfREcHsqOtjItmicEXcwByG6UkGhm\" width=\"500\"/>\n","</div>\n","[그림출처:https://www.sharpsightlabs.com/blog/numpy-relu/]\n","\n","<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=1brqTic1gberwaEo15QG2PylvVOLktZcw\" width=\"400\"/>\n","</div>\n","[그림출처:https://pyimagesearch.com/2021/05/14/convolutional-neural-networks-cnns-and-layer-types/]\n","\n","<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=1c8R6EKdEWa2hVbEYlBIWAlbW6jhXuaKl\" width=\"600\"/>\n","</div>\n","[그림출처:https://www.cuemath.com/algebra/addition-of-matrices/]\n"],"metadata":{"id":"h9CF2i3GDaA5"}},{"cell_type":"markdown","source":["If you want to write a naive Python implementation of an element-wise operation, you use a for loop, \n","\n","as in this **naive implementation of an element-wise relu operation**:"],"metadata":{"id":"V_77hNsxDZKX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SYothK1OnkJ"},"outputs":[],"source":["def naive_relu(x):\n","    assert len(x.shape) == 2                     #  x is a 2D Numpy tensor.\n","\n","    x = x.copy()                                 #  Avoid overwriting the input tensor.\n","    for i in range(x.shape[0]):\n","        for j in range(x.shape[1]):\n","            x[i, j] = max(x[i, j], 0)\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"H6tEwUmPOnkK"},"source":["You do the same for **addition**:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dmq49SIDOnkK"},"outputs":[],"source":["def naive_add(x, y):\n","    assert len(x.shape) == 2\n","    assert x.shape == y.shape                # x and y are 2D Numpy tensors.\n","\n","    x = x.copy()\n","    for i in range(x.shape[0]):\n","        for j in range(x.shape[1]):\n","            x[i, j] += y[i, j]\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"ZU4HnLEBOnkK"},"source":["On the same principle, you can do **element-wise multiplication**, **subtraction**, and so on."]},{"cell_type":"markdown","metadata":{"id":"4urE5DX3OnkK"},"source":["- In practice, when dealing with Numpy arrays, these operations are available as well-optimized **built-in Numpy functions**\n","\n","- So, in Numpy, you can do the following element-wise operation, and it will be blazing fast:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4F91v567OnkL"},"outputs":[],"source":["import numpy as np\n","\n","z = x + y                       # Element-wise Addition\n","\n","z = np.maximum(z, 0.)           # Element-wise relu"]},{"cell_type":"markdown","metadata":{"id":"mR8mVsMQOnkL"},"source":["## 2.3.2. Broadcasting\n","\n","- Our earlier naive implementation of naive_add only supports \n","  the addition of **2D tensors with identical shapes**.  \n","  (앞서 살펴본 단순한 덧셈 구현인 naive_add는 **동일한 크기**의 2D 텐서만 지원함)\n","- But in the **Dense layer** introduced earlier, <font color=\"blue\">**we added a 2D tensor with a vector**</font>.  \n","(하지만 이전에 보았던 **Dense 층에서는 2D 텐서와 벡터를 더했음**)\n","\n","### **What happens with addition when the shapes of the two tensors being added differ?**  \n","**크기가 다른 두 텐서가 더해질 때 무슨 일이 일어날까요?**"]},{"cell_type":"markdown","metadata":{"id":"VaGTGsLHOnkL"},"source":["When possible, and if there’s no ambiguity, \n","\n","- the smaller tensor will be broadcasted to match the shape of the larger tensor.  \n","(<font color=\"blue\">**작은 텐서가 큰 텐서의 크기에 맞추어 브로드캐스팅(broadcasting)됨**</font>)\n","\n","### Broadcasting consists of two steps:\n","\n","1. Axes (called broadcast axes) are added to the smaller tensor to match the ndim of the larger tensor.  \n","(**큰 텐서의 ndim에 맞도록 작은 텐서에 축이 추가**됨. 이 추가된 새 축을 **브로드캐스팅 축**이라고 함)\n","1. The smaller tensor is repeated alongside these new axes to match the full shape of the larger tensor.  \n","(**작은 텐서가 새 축을 따라서 큰 텐서의 크기에 맞도록 반복됨**) "]},{"cell_type":"markdown","source":["<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=1SDbse23OO0gdJ2kK-4tTGa2WhPkdEmyd\" width=\"500\"/>\n","</div>\n","[그림출처:https://medium.com/analytics-vidhya/pytorch-for-deep-learning-part-2-bc0cfa12e74]\n"],"metadata":{"id":"PtBz2J8uHVqS"}},{"cell_type":"markdown","metadata":{"id":"-1c08AL7OnkL"},"source":["### Let’s look at a concrete example. \n","\n","- Consider X with shape (32, 10) and y with shape (10,). \n","- First, we add an empty first axis to y, whose shape becomes (1, 10). \n","- Then, we repeat y 32 times alongside this new axis, so that we end up with a tensor Y with shape (32, 10), where Y[i, :] == y for i in range(0, 32). \n","- At this point, we can proceed to add X and Y, because they have the same shape."]},{"cell_type":"markdown","metadata":{"id":"GtGrWsQZOnkM"},"source":["\n","### Here’s what a naive implementation would look like:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eJU0gbNmOnkM"},"outputs":[],"source":["def naive_add_matrix_and_vector(x, y):\n","    assert len(x.shape) == 2                           #  x is a 2D Numpy tensor\n","    assert len(y.shape) == 1                           #  y is a Numpy vector.\n","    assert x.shape[1] == y.shape[0]\n","\n","    x = x.copy()                                       #  Avoid overwriting the input tensor\n","    for i in range(x.shape[0]):\n","        for j in range(x.shape[1]):\n","            x[i, j] += y[j]\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"D0nqkhJ6OnkM"},"source":["- With broadcasting, you can generally apply two-tensor element-wise operations if one tensor has shape  \n","(**a, b, ... , n-1**, n, n + 1, ... , m) and the other has shape (n, n + 1, ..., m). \n","\n","- The broadcasting will then automatically happen for axes **a through n - 1**."]},{"cell_type":"markdown","metadata":{"id":"EfriG9kUOnkM"},"source":["### The following example applies the element-wise maximum operation to two tensors of different shapes via broadcasting:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDDIRWxwOnkN"},"outputs":[],"source":["import numpy as np\n","\n","x = np.random.random((64, 3, 32, 10))        # x is a random tensor with shape (64, 3, 32, 10)\n","y = np.random.random((32, 10))              #  y is a random tensor with shape (32, 10).\n","\n","z = np.maximum(x, y)                        # The output z has shape (64, 3, 32, 10) like x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O0h4UMBoOnkN","executionInfo":{"status":"ok","timestamp":1640765308753,"user_tz":-540,"elapsed":3,"user":{"displayName":"와이엇w","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv7EoU9__RjwJXI2BeNCGOqZDaP_YO-qYfbY977Q=s64","userId":"17828975708330147939"}},"outputId":"2d8b7040-6b32-412b-fc34-99d7519f5f46"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(64, 3, 32, 10)"]},"metadata":{},"execution_count":8}],"source":["z.shape"]},{"cell_type":"markdown","metadata":{"id":"7xYS2rVJOnkN"},"source":["## 2.3.3. Tensor dot (텐서 점곱)\n","\n","- The **dot operation**, also called a **tensor product** (not to be confused with an element-wise product) is the most common, most useful tensor operation.  \n","(텐서 곱셈(tensor product)라고도 부르는 점곱 연산(dot operation)은 가장 널리 사용되고 유용한 텐서 연산임)\n","- Contrary to element-wise operations, <font color=\"blue\">**it combines entries in the input tensors**</font>.  \n","(원소별 연산과 반대로 입력 텐서의 원소들을 겹합시킴)"]},{"cell_type":"markdown","metadata":{"id":"yug9MG76OnkN"},"source":["- An element-wise product is done with the * operator in Numpy, Keras, Theano, and TensorFlow. \n","- dot uses a different syntax in TensorFlow, but in both Numpy and Keras it’s done using the standard dot operator:\n","\n","@ 텐서플로에서는 tf.matmul(x, y)처럼 사용함."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4D66G7-gOnkN"},"outputs":[],"source":["import numpy as np\n","z = np.dot(x, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFgR_vuAOnkN"},"outputs":[],"source":["from keras import backend as K\n","z = K.dot(x, y)"]},{"cell_type":"markdown","metadata":{"id":"3fmIxYT9OnkN"},"source":[":In mathematical notation, you’d note the operation with a dot (.)"]},{"cell_type":"markdown","metadata":{"id":"eFG0E6LoOnkO"},"source":["z = x . y"]},{"cell_type":"markdown","metadata":{"id":"6gBLm0SxOnkO"},"source":["Mathematically, what does the dot operation do? \n","Let’s start with the dot product of two vectors x and y. \n","\n","It’s computed as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kp57CWAOnkO"},"outputs":[],"source":["def naive_vector_dot(x, y):\n","    assert len(x.shape) == 1\n","    assert len(y.shape) == 1\n","    assert x.shape[0] == y.shape[0]\n","\n","    z = 0.\n","    for i in range(x.shape[0]):\n","        z += x[i] * y[i]\n","    return z\n","     \n","    # x and y are Numpy vectors."]},{"cell_type":"markdown","metadata":{"id":"M6vjRxpcOnkO"},"source":["- The dot product between two vectors is a **scalar**.  \n","(두 벡터의 dot product는 스칼라가 됨)\n","- Only vectors with **the same number** of elements are compatible for a dot product.  \n","(원소 개수가 같은 벡터끼리 dot product가 가능함)"]},{"cell_type":"markdown","metadata":{"id":"9EyxGvN0OnkO"},"source":["You can also take **the dot product between a matrix x and a vector y**, which returns **a vector where the coefficients are the dot products between y and the rows of x**.  \n","(행렬 x와 벡터 y 사이에서도 dot product가 가능함  \n"," y와 x의 행 사이에서 dot product가 일어나므로 벡터가 반환됨)\n","\n","\n","You implement it as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2W2oST1OnkO"},"outputs":[],"source":["import numpy as np\n","\n","def naive_matrix_vector_dot(x, y):\n","    assert len(x.shape) == 2\n","    assert len(y.shape) == 1\n","    assert x.shape[1] == y.shape[0]\n","\n","    z = np.zeros(x.shape[0])\n","    for i in range(x.shape[0]):\n","        for j in range(x.shape[1]):\n","            z[i] += x[i, j] * y[j]\n","    return z"]},{"cell_type":"markdown","metadata":{"id":"BWjoyguFOnkO"},"source":["You could also reuse the code we wrote previously, which highlights the relationship between a matrix-vector product and a vector product:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_uAsQhhTOnkO"},"outputs":[],"source":["def naive_matrix_vector_dot(x, y):\n","    z = np.zeros(x.shape[0])\n","    for i in range(x.shape[0]):\n","        z[i] = naive_vector_dot(x[i, :], y)\n","    return z"]},{"cell_type":"markdown","metadata":{"id":"Z1OtG6eVOnkP"},"source":["Note that as soon as one of the two tensors has an ndim greater than 1, \n","\n","dot is no longer symmetric (commutative), which is to say that <font color=\"blue\">**dot(x, y) isn’t the same as dot(y, x)**</font>.  \n","(두 텐서 중 하나라도 ndim이 1보다 크면 dot 연산에 교환 법칙이 성립되지 않음. 다시 말하면, dot(x,y)와 dot(y,x)가 일반적으로 같지 않음.)"]},{"cell_type":"markdown","metadata":{"id":"A4EcLmnEOnkP"},"source":["- Of course, a dot product generalizes to tensors with an arbitrary number of axes.  \n","(dot product는 임의의 축 개수를 가진 텐서에 일반화됨)\n","- The most common applications may be the dot product between two matrices.  \n","(가장 일반적인 용도는 두 행렬 간의 dot product임)\n","- You can take the dot product of two matrices x and y (dot(x, y)) if and only if x.shape[1] == y.shape[0].  \n","(x.shape[1] == y.shape[0]일 때, 두 행렬 x와 y의 dot product (dot(x, y))가 성립함)\n","- The result is a matrix with shape (x.shape[0], y.shape[1]), where the coefficients are the vector products between the rows of x and the columns of y.  \n","(x의 열과 y의 행 사이 벡터 dot product로 인해 (x.shape[0], y.shape[1]) 크기의 행렬이 됨)\n","\n","Here’s the naive implementation:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YsAYqMG7OnkP"},"outputs":[],"source":["def naive_matrix_dot(x, y):\n","    assert len(x.shape) == 2\n","    assert len(y.shape) == 2\n","    assert x.shape[1] == y.shape[0]\n","\n","    z = np.zeros((x.shape[0], y.shape[1]))\n","    for i in range(x.shape[0]):\n","        for j in range(y.shape[1]):\n","            row_x = x[i, :]\n","            column_y = y[:, j]\n","            z[i, j] = naive_vector_dot(row_x, column_y)\n","    return z"]},{"cell_type":"markdown","metadata":{"id":"gJ90P6o6OnkP"},"source":["To understand dot-product shape compatibility, it helps to visualize the input and output tensors by aligning them as shown in figure 2.5."]},{"cell_type":"markdown","source":["\n","<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=1GQOqhqNV42cZUPuwLcgd0BgXH5gcu516\" width=\"600\"/>\n","</div>"],"metadata":{"id":"qqJTU47gE8LE"}},{"cell_type":"markdown","metadata":{"id":"3rxr2-DnOnkP"},"source":["- x, y, and z are pictured as rectangles (literal boxes of coefficients). \n","- Because the rows x and the columns of y must have the same size, it follows that the width of x must match the height of y."]},{"cell_type":"markdown","metadata":{"id":"8HR-8XxbOnkP"},"source":["More generally, you can take the dot product between higher-dimensional tensors, following the same rules for shape compatibility as outlined earlier for the 2D case  \n","(더 일반적으로는 앞서 설명한 2D의 경우처럼 크기를 맞추는 동일한 규칙을 따르면 다음과 같이 고차원 텐서 간의 dot product을 할 수 있음):"]},{"cell_type":"markdown","metadata":{"id":"cmPmqFsYOnkP"},"source":["<font color=\"blue\">**(a, b, c, d) . (d,) -> (a, b, c)**</font>\n","\n","<font color=\"blue\">**(a, b, c, d) . (d, e) -> (a, b, c, e)**</font>"]},{"cell_type":"markdown","metadata":{"id":"EOgAmmGgOnkP"},"source":["## 2.3.4. Tensor reshaping\n","\n","-  It is essential to understand is **tensor reshaping (텐서 크기 변환)**. \n","- Although it wasn’t used in the Dense layers in our first neural network example, we used it when we preprocessed the digits data before feeding it into our network:  \n","(2.1 예제에서 신경망에 주입할 숫자 데이터를 전처리할 때 사용함)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kab7b7y8OnkQ","executionInfo":{"status":"ok","timestamp":1640765395578,"user_tz":-540,"elapsed":1059,"user":{"displayName":"와이엇w","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv7EoU9__RjwJXI2BeNCGOqZDaP_YO-qYfbY977Q=s64","userId":"17828975708330147939"}},"outputId":"33369a28-93fa-4dc4-ad40-876c3e03bb3e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n","11501568/11490434 [==============================] - 0s 0us/step\n"]}],"source":["from keras.datasets import mnist\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EKDydEu8OnkQ","executionInfo":{"status":"ok","timestamp":1640765398195,"user_tz":-540,"elapsed":418,"user":{"displayName":"와이엇w","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv7EoU9__RjwJXI2BeNCGOqZDaP_YO-qYfbY977Q=s64","userId":"17828975708330147939"}},"outputId":"a57fe237-bf24-4a4e-edc5-c22071879a31"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 28, 28)"]},"metadata":{},"execution_count":16}],"source":["train_images.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9SLGzXqOnkQ"},"outputs":[],"source":["train_images = train_images.reshape((60000, 28 * 28))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plGysnicOnkQ","executionInfo":{"status":"ok","timestamp":1640765401982,"user_tz":-540,"elapsed":3,"user":{"displayName":"와이엇w","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv7EoU9__RjwJXI2BeNCGOqZDaP_YO-qYfbY977Q=s64","userId":"17828975708330147939"}},"outputId":"098a3e61-233f-4c3d-a4d6-353d808712dd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(60000, 784)"]},"metadata":{},"execution_count":18}],"source":["train_images.shape"]},{"cell_type":"markdown","metadata":{"id":"i4WUe1HAOnkQ"},"source":["- **Reshaping a tensor means rearranging its rows and columns to match a target shape**.  \n","(<font color=\"blue\">텐서의 크기를 변환한다는 것은 특정 크기에 맞게 열과 행을 재배열한다는 뜻임</font>) \n","- Naturally, the reshaped tensor has the same total number of coefficients as the initial tensor.  \n","(<font color=\"blue\">변환된 텐서는 원래 텐서와 원소의 개수가 동일함.</font>)\n","\n","Reshaping is best understood via simple examples:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NS5VFSzrOnkQ","executionInfo":{"status":"ok","timestamp":1640765404474,"user_tz":-540,"elapsed":613,"user":{"displayName":"와이엇w","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv7EoU9__RjwJXI2BeNCGOqZDaP_YO-qYfbY977Q=s64","userId":"17828975708330147939"}},"outputId":"94ec9e3b-c50b-40fa-9ba0-988a174312c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["(3, 2)\n"]}],"source":["import numpy as np\n","x = np.array([[0., 1.],\n","              [2., 3.],\n","              [4., 5.]])\n","print(x.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q6Uu7EkGOnkQ","executionInfo":{"status":"ok","timestamp":1640765405801,"user_tz":-540,"elapsed":3,"user":{"displayName":"와이엇w","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv7EoU9__RjwJXI2BeNCGOqZDaP_YO-qYfbY977Q=s64","userId":"17828975708330147939"}},"outputId":"69f7acdb-54c0-4d20-9942-231b6c11aaf0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.],\n","       [1.],\n","       [2.],\n","       [3.],\n","       [4.],\n","       [5.]])"]},"metadata":{},"execution_count":20}],"source":["x = x.reshape((6, 1))\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"3PncMJOwOnkQ","executionInfo":{"status":"ok","timestamp":1640765407626,"user_tz":-540,"elapsed":2,"user":{"displayName":"와이엇w","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv7EoU9__RjwJXI2BeNCGOqZDaP_YO-qYfbY977Q=s64","userId":"17828975708330147939"}},"outputId":"34635f94-c6ef-44b0-c239-b8cbfec01cb0"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(6, 1)"]},"metadata":{},"execution_count":21}],"source":["x.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xEf2XsmjOnkR","executionInfo":{"status":"ok","timestamp":1640765409235,"user_tz":-540,"elapsed":2,"user":{"displayName":"와이엇w","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv7EoU9__RjwJXI2BeNCGOqZDaP_YO-qYfbY977Q=s64","userId":"17828975708330147939"}},"outputId":"c4533c92-9bbf-42c0-a9d6-435d0c40b9eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 1., 2.],\n","       [3., 4., 5.]])"]},"metadata":{},"execution_count":22}],"source":["x = x.reshape((2, 3))\n","x"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d8aTaGS2OnkT","executionInfo":{"status":"ok","timestamp":1640765410807,"user_tz":-540,"elapsed":3,"user":{"displayName":"와이엇w","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv7EoU9__RjwJXI2BeNCGOqZDaP_YO-qYfbY977Q=s64","userId":"17828975708330147939"}},"outputId":"0c0e83af-fa59-406f-81c4-81f313bf074d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 3)"]},"metadata":{},"execution_count":23}],"source":["x.shape"]},{"cell_type":"markdown","metadata":{"id":"tDKBUQAuOnkU"},"source":["- A special case of reshaping that’s commonly encountered is **transposition**.  \n","(자주 사용하는 특별한 크기 변환은 전치(transposition)임)\n","- Transposing a matrix means **exchanging its rows and its columns**, so that x[i, :] becomes x[:, i]:  \n","(행렬의 전치는 행과 열을 바꾸는 것을 의미함. 즉, x[i, :]이 x[:, i]가 됨)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qnvmQHlHOnkU","executionInfo":{"status":"ok","timestamp":1640765413981,"user_tz":-540,"elapsed":422,"user":{"displayName":"와이엇w","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggv7EoU9__RjwJXI2BeNCGOqZDaP_YO-qYfbY977Q=s64","userId":"17828975708330147939"}},"outputId":"3a0d8984-7212-4885-c2e0-a34d7871fbf6"},"outputs":[{"output_type":"stream","name":"stdout","text":["(20, 300)\n"]}],"source":["x = np.zeros((300, 20))\n","\n","x = np.transpose(x)\n","\n","print(x.shape)"]},{"cell_type":"markdown","metadata":{"id":"yJ3yHLfDOnkU"},"source":["## 2.3.5. Geometric interpretation of tensor operations\n","\n","- Because the contents of the tensors manipulated by tensor operations can be interpreted as coordinates of points in some geometric space, **all tensor operations have a geometric interpretation**.  \n","(<font color=\"blue\">텐서 연산이 조작하는 **텐서의 내용**은 **어떤 기하학적 공간에 있는 좌표 포인트**로 해석될 수 있기 때문에  \n"," **모든 텐서 연산은 기하학적 해석이 가능**함.</font>) \n","- For instance, let’s consider addition. We’ll start with the following vector:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SzzGig0dOnkU"},"outputs":[],"source":["A = [0.5, 1]"]},{"cell_type":"markdown","metadata":{"id":"a-UBhawsOnkU"},"source":["It’s a point in a 2D space (see figure 2.6). It’s common to picture a vector as an arrow linking the origin to the point, as shown in figure 2.7."]},{"cell_type":"markdown","source":["<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=1osN0uZTXZRBplCAUQF-hrSoov6KPwbEu\" width=\"600\"/>\n","</div>"],"metadata":{"id":"gvmyP_llHEYW"}},{"cell_type":"markdown","source":["<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=1ivvabBIiG61w4Xg4A-RhmsHpwzua2WSL\" width=\"600\"/>\n","</div>"],"metadata":{"id":"vtS_u-G7HOll"}},{"cell_type":"markdown","metadata":{"id":"Y_hlaHJnOnkU"},"source":["- Let’s consider a new point, B = [1, 0.25], which we’ll add to the previous one. \n","- This is done geometrically by chaining together the vector arrows, with the resulting location being the vector representing the sum of the previous two vectors (see figure 2.8).  \n","(기하학적으로 벡터 화살표를 연결하여 계산할 수 있음. 최종 위치는 두 벡터의 덧셈을 나타내는 벡터가 됨)"]},{"cell_type":"markdown","source":["<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=1JYlST6gCaSI2YOhf8po_eDuXBhZVk9UC\" width=\"600\"/>\n","</div>"],"metadata":{"id":"FTDh0OmQHapS"}},{"cell_type":"markdown","metadata":{"id":"mitA8X18OnkV"},"source":["- In general, **elementary geometric operations** such as affine transformations, rotations, scaling, and so on can be expressed as **tensor operations.**  \n","(일반적으로 아핀 변환(affine transformation), 회전, 스케일링 등 처럼 <font color=\"blue\">**기본적인 기하학적 연산은 텐서 연산으로 표현될 수 있음**</font>)\n","- For instance, a rotation of a 2D vector by an angle theta can be achieved via a dot product with a 2 × 2 matrix R = [u, v], where u and v are both vectors of the plane: u = [cos(theta), sin(theta)] and v = [-sin(theta), cos(theta)].  \n","(예를 들어, theta 각도로 2D 벡터를 회전하는 것은 2x2 행렬 R = [u, v]를 dot product하여 구현할 수 있음. 여기에서 u 와 v 는 동일 평면상의 벡터이며, u = [cos(theta), sin(theta)] 고 v = [-sin(theta), cos(theta)]임)"]},{"cell_type":"markdown","metadata":{"id":"fFtzJL26OnkV"},"source":["## 2.3.6. A geometric interpretation of deep learning\n","\n","You just learned that \n","\n","- **Neural networks consist entirely of chains of tensor operations**.  \n","(신경망은 전체적으로 텐서 연산의 연결로 구성됨)\n","- All of these tensor operations are just **geometric transformations of the input data**.  \n","(모든 텐서 연산은 입력 데이터의 기하학적 변환임)\n","- It follows that you can interpret a neural network as **a very complex geometric transformation in a high-dimensional space**, implemented via a long series of simple steps.  \n","(단순한 단계들이 길게 이어져 구현된 신경망을 <font color=\"blue\">**고차원 공간에서 매우 복잡한 기하학적 변환**</font>을 하는 것으로 해석 가능)"]},{"cell_type":"markdown","metadata":{"id":"JbFzp53YOnkV"},"source":["In 3D, the following mental image may prove useful. \n","\n","- Imagine two sheets of colored paper: one red and one blue. \n","- Put one on top of the other. \n","- Now crumple them together into a small ball. \n","- That crumpled paper ball is your input data, and each sheet of paper is a class of data in a classification problem. \n","- What a neural network (or any other machine-learning model) is meant to do is figure out a transformation of the paper ball that would uncrumple it, so as to make the two classes cleanly separable again. \n","- With deep learning, this would be implemented as a series of simple transformations of the 3D space, such as those you could apply on the paper ball with your fingers, one movement at a time."]},{"cell_type":"markdown","metadata":{"id":"0Q3-PB57OnkV"},"source":["Figure 2.9. Uncrumpling a complicated manifold of data\n","<div>\n","<img src=\"https://drive.google.com/uc?export=view&id=12gC5q90kT5vOo_N0l0rujv65xqExIvGg\" width=\"800\"/>\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"eNmxWAwPOnkV"},"source":["- Uncrumpling paper balls is what machine learning is about: **finding neat representations for complex, highly folded data manifolds**. (즉, 머신러닝 : 복잡하고 심하게 꼬여 있는 데이터의 manifold에 대한 깔끔한 표현을 찾는 일을 함.)\n","- At this point, you should have a pretty good intuition as to why deep learning excels at this: it takes the approach of incrementally decomposing a complicated geometric transformation into a long chain of elementary ones, which is pretty much the strategy a human would follow to uncrumple a paper ball.(딥러닝이 뛰어난 이유는 기초적인 연산을 길게 연결하여 복잡한 기하학적 변환을 조금씩 분해하는 방식이 마치 사람이 종이 공을 펼치기 위한 전략과 매우 흡사하기 때문임.) \n","- Each layer in a deep network applies a transformation that disentangles the data a little—and a deep stack of layers makes tractable an extremely complicated disentanglement process.(심층 네트워크의 각 층은 데이터를 조금씩 풀어 주는 변환을 적용하므로, 이런 층을 깊게 쌓으면 아주 복잡한 분해 과정을 처리할 수 있다.)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"colab":{"provenance":[],"collapsed_sections":["-1c08AL7OnkL"]}},"nbformat":4,"nbformat_minor":0}